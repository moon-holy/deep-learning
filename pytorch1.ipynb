{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空所有的变量\n",
    "%reset -f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 列表转tensor\n",
    "a = [1, 3, 5]\n",
    "b = torch.Tensor(a)\n",
    "display(b, b.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 元组转tensor\n",
    "d = torch.tensor(((1, 2), (3, 4)))\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用numpy转tensor\n",
    "torch.tensor(np.arange(15).reshape(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty api\n",
    "torch.empty([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ones api\n",
    "torch.ones([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = torch.ones_like(torch.ones([3,4]))\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros api\n",
    "torch.zeros([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# zeros_like api\n",
    "d = torch.zeros_like(torch.zeros([3,4]))\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eye api(单位矩阵)\n",
    "d = torch.eye(2, 2)\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94651651, 0.8413694 , 0.69195422],\n",
       "       [0.31956688, 0.58783865, 0.97428372]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9465, 0.8414, 0.6920],\n",
       "        [0.3196, 0.5878, 0.9743]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 高斯分布的tensor\n",
    "a = np.random.rand(2, 3)\n",
    "display(a, a.dtype)\n",
    "b = torch.Tensor(a)\n",
    "display(b, b.type, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1420, 0.6133, 0.9628],\n",
       "        [0.6327, 0.8919, 0.3087]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor的高斯分布api\n",
    "d = torch.rand(2, 3)\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4989, -1.2839, -0.1729],\n",
       "        [-2.1871, -0.2828,  0.0365]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4989, -1.2839, -0.1729],\n",
       "        [-2.1871, -0.2828,  0.0365]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建一个mean = 0, std = 1的高斯分布\n",
    "dd = torch.normal(mean=0, std=1, size=(2, 3), out = b)\n",
    "display(b)\n",
    "display(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2582, -0.8671],\n",
       "        [ 0.6809, -0.8624]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用uniform创建一个-1到1的均匀分布的随机数\n",
    "d = torch.Tensor(2, 2).uniform_(-1, 1)\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 8, 9, 5, 2, 1, 7, 4, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 返回从0到指定范围的随机数\n",
    "d = torch.randperm(10)\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用torch的arange\n",
    "a = torch.arange(start=1, end=10, step=3)\n",
    "display(a, a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  6.,  2.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 序列tensor\n",
    "d = torch.linspace(10, 2, 3)\n",
    "display(d)\n",
    "display(d.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3, 3],\n",
      "        [3, 3, 3]]) <built-in method type of Tensor object at 0x0000028A540E2F90> torch.int64\n"
     ]
    }
   ],
   "source": [
    "# torch.full\n",
    "a = torch.full((2, 3), 3)\n",
    "print(a, a.type, a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7825, 0.7358]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 随机种子\n",
    "torch.manual_seed(1)\n",
    "mean = torch.rand(1, 2)\n",
    "std  = torch.rand(1, 2)\n",
    "display(torch.normal(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor的属性和方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取tensor的值\n",
    "a = torch.tensor([[[1]]])\n",
    "display(a)\n",
    "display(a.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转成numpy\n",
    "t = a.numpy()\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取其大小\n",
    "display(a.shape, a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29]]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取某个维度的值\n",
    "a = torch.tensor(np.arange(30).reshape(3,2,5))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取第一个维度的样本量\n",
    "display(a.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]],\n",
       "\n",
       "        [[15, 16, 17, 18, 19],\n",
       "         [20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对tensor进行视图（转成2，3，5） 不改变内存，只改变步长和形状\n",
    "a.view([2,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  6, 11],\n",
       "        [ 2,  7, 12],\n",
       "        [ 3,  8, 13],\n",
       "        [ 4,  9, 14]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 进行维度交换\n",
    "b = torch.tensor(np.arange(15).reshape(3,5))\n",
    "display(b)\n",
    "# 进行维度交换\n",
    "display(torch.t(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  6, 11],\n",
       "        [ 2,  7, 12],\n",
       "        [ 3,  8, 13],\n",
       "        [ 4,  9, 14]], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接交换行和列\n",
    "b.transpose(0, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1643],\n",
       "         [0.0967],\n",
       "         [0.1936]],\n",
       "\n",
       "        [[0.7837],\n",
       "         [0.9381],\n",
       "         [0.7215]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1643, 0.0967, 0.1936],\n",
       "        [0.7837, 0.9381, 0.7215]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1643]],\n",
       "\n",
       "         [[0.0967]],\n",
       "\n",
       "         [[0.1936]]],\n",
       "\n",
       "\n",
       "        [[[0.7837]],\n",
       "\n",
       "         [[0.9381]],\n",
       "\n",
       "         [[0.7215]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.squeeze 用于从张量中移除维度为 1 的维度\n",
    "# torch.unsqueeze 用于在张量中插入维度为 1 的维度\n",
    "a = torch.rand(2, 3, 1)\n",
    "display(a)\n",
    "out = torch.squeeze(a)\n",
    "display(out)\n",
    "display(out.shape)\n",
    "out = torch.unsqueeze(a, -1)\n",
    "display(out)\n",
    "display(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1643, 0.0967, 0.1936],\n",
       "         [0.7837, 0.9381, 0.7215]]),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.unbind 是 PyTorch 中用于在指定维度上解绑张量的函数,并返回元组\n",
    "out = torch.unbind(a, dim=2)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1825, 0.1139, 0.5893],\n",
       "        [0.2744, 0.1487, 0.4156]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4156, 0.1487, 0.2744],\n",
       "        [0.5893, 0.1139, 0.1825]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 沿着指定维度翻转（反转）张量的元素顺序\n",
    "a = torch.rand(2, 3)\n",
    "display(a)\n",
    "display(torch.flip(a, dims=[1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于将张量沿着指定维度进行逆时针旋转90度，旋转k次\n",
    "a = torch.rand(2, 3)\n",
    "out = torch.rot90(a, -1, dims=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把b展开之后等分为5分\n",
    "b.view(5,-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 8, 9], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 8, 9], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  6, 11], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取tensor某个维度的值\n",
    "display(b[1, :])\n",
    "display(b[1])\n",
    "display(b[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2,   3,   4],\n",
       "        [  5, 100,   7,   8,   9],\n",
       "        [ 10, 100,  12,  13,  14]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 以广播的形式为选中元素赋值\n",
    "b[:, 1] = 100\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2,   3,   4],\n",
       "        [  5, 100,   7,   8,   9],\n",
       "        [ 10, 100,  12,  13, 100]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 为单个元素赋值\n",
    "b[-1, -1] = 100\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 转换tensor的数据格式\n",
    "display(torch.FloatTensor([1,2,3]).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初始化tensor是设置数据格式\n",
    "display(torch.ones([2,3],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对原有tensor进行初始化\n",
    "display(b.new_ones(size=b.size()))\n",
    "display(b.zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 以广播的形式对tensor进行+1\n",
    "display(b.add(1))  # 复制+\n",
    "display(b.add_(1)) # 原地+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuda类型的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "display(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将cpu的tensor转到gpu上\n",
    "a = torch.tensor([1, 1, 1], dtype=float)\n",
    "display(a.device)\n",
    "a = a.to(device)\n",
    "display(a.device)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将cuda转到cpu\n",
    "display(a.cpu().device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cpu上的tensor和gpu上的tensor进行+\n",
    "b = torch.tensor([0,0,0])\n",
    "display(a + b.to(device)) # 会到gpu上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor的属性和方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1095, 9.5109],\n",
       "        [9.5415, 1.6618]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5.],\n",
       "        [5., 2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clamp对tensor数据设置取值范围\n",
    "a = torch.rand(2, 2) * 10\n",
    "display(a)\n",
    "a = a.clamp(2, 5)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5350, 0.8068],\n",
       "        [0.9854, 0.1264]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3737, 0.4212]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9087, 1.2280],\n",
       "        [1.3591, 0.5476]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor的广播 加到每一行\n",
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(1, 2)\n",
    "c = a + b\n",
    "display(a)\n",
    "display(b)\n",
    "display(c)\n",
    "display(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0262, 0.1459, 0.9889],\n",
       "        [0.5878, 0.7082, 0.6546]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4083, 0.6137, 0.1125],\n",
       "        [0.8795, 0.1412, 0.1745]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [False,  True,  True]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [False,  True,  True]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False],\n",
       "        [ True, False, False]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False],\n",
       "        [ True, False, False]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor之间的比较\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "display(a, b)\n",
    "\n",
    "display(torch.eq(a, b))\n",
    "display(torch.equal(a, b))\n",
    "\n",
    "display(torch.ge(a, b))  # >=\n",
    "display(torch.gt(a, b))  # >\n",
    "display(torch.le(a, b))  # <=\n",
    "display(torch.lt(a, b))  # <\n",
    "display(torch.ne(a, b))  # !="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[1, 3, 4, 4, 5],\n",
       "        [1, 2, 3, 3, 5]]),\n",
       "indices=tensor([[0, 3, 1, 2, 4],\n",
       "        [2, 0, 1, 3, 4]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor的排序\n",
    "a = torch.tensor([[1, 4, 4, 3, 5],\n",
    "                  [2, 3, 1, 3, 5]])\n",
    "display(a.shape)\n",
    "display(torch.sort(a, dim=1, descending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[1, 2],\n",
       "        [1, 2]]),\n",
       "indices=tensor([[3, 0],\n",
       "        [3, 0]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查找前tensor的最大(小)的k个值，并返回下标\n",
    "a = torch.tensor([[2, 4, 3, 1, 5],\n",
    "                  [2, 3, 5, 1, 4]])\n",
    "display(a.shape)\n",
    "display(torch.topk(a, k = 2, dim=1, largest=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([2, 4, 5, 1, 5]),\n",
       "indices=tensor([1, 0, 1, 1, 0]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([2, 2]),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 第k小（或第k大）的值，而不返回对应的索引。\n",
    "display(torch.kthvalue(a, k=2, dim=0))\n",
    "display(torch.kthvalue(a, k=2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4513, 0.8150, 0.1886],\n",
      "        [0.2309, 0.1703, 0.5045]])\n",
      "tensor([[inf, inf, inf],\n",
      "        [inf, inf, inf]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# tensor的取值\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(a/0)\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isfinite(a/0))\n",
    "print(torch.isinf(a/0))\n",
    "print(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., nan])\n",
      "tensor([False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "# 判断是否为numpy中的非数值\n",
    "a = torch.tensor([1, 2, np.nan])\n",
    "print(a)\n",
    "print(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9023, 0.7209, 0.6406, 0.3282],\n",
       "        [0.6025, 0.8401, 0.1511, 0.8847],\n",
       "        [0.5466, 0.8626, 0.6166, 0.8416]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9023, 0.7209],\n",
       "        [0.6025, 0.8401],\n",
       "        [0.5466, 0.8626]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6406, 0.3282],\n",
       "        [0.1511, 0.8847],\n",
       "        [0.6166, 0.8416]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用于将输入张量沿着指定维度进行分块\n",
    "a = torch.rand((3, 4))\n",
    "display(a)\n",
    "out = torch.chunk(a, 2, dim=1)\n",
    "display(out[0], out[0].shape)\n",
    "display(out[1], out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9251, 0.3730, 0.3715, 0.8856],\n",
       "        [0.5826, 0.2469, 0.8072, 0.5046],\n",
       "        [0.0789, 0.4340, 0.6834, 0.8746],\n",
       "        [0.0714, 0.9318, 0.5716, 0.3496],\n",
       "        [0.0279, 0.5919, 0.7209, 0.3265],\n",
       "        [0.0662, 0.4997, 0.9798, 0.1893],\n",
       "        [0.1890, 0.4692, 0.3342, 0.7685],\n",
       "        [0.8841, 0.5261, 0.2210, 0.0932],\n",
       "        [0.2867, 0.0605, 0.8225, 0.1724],\n",
       "        [0.7335, 0.7447, 0.5862, 0.8193]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9251, 0.3730, 0.3715, 0.8856],\n",
       "        [0.5826, 0.2469, 0.8072, 0.5046],\n",
       "        [0.0789, 0.4340, 0.6834, 0.8746]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0714, 0.9318, 0.5716, 0.3496],\n",
       "        [0.0279, 0.5919, 0.7209, 0.3265],\n",
       "        [0.0662, 0.4997, 0.9798, 0.1893]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1890, 0.4692, 0.3342, 0.7685],\n",
       "        [0.8841, 0.5261, 0.2210, 0.0932],\n",
       "        [0.2867, 0.0605, 0.8225, 0.1724]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7335, 0.7447, 0.5862, 0.8193]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9251, 0.3730, 0.3715, 0.8856]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5826, 0.2469, 0.8072, 0.5046],\n",
       "        [0.0789, 0.4340, 0.6834, 0.8746],\n",
       "        [0.0714, 0.9318, 0.5716, 0.3496]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0279, 0.5919, 0.7209, 0.3265],\n",
       "        [0.0662, 0.4997, 0.9798, 0.1893],\n",
       "        [0.1890, 0.4692, 0.3342, 0.7685],\n",
       "        [0.8841, 0.5261, 0.2210, 0.0932],\n",
       "        [0.2867, 0.0605, 0.8225, 0.1724],\n",
       "        [0.7335, 0.7447, 0.5862, 0.8193]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.rand((10, 4))\n",
    "display(a)\n",
    "out = torch.split(a, 3, dim=0)\n",
    "display(len(out))\n",
    "for t in out:\n",
    "    display(t, t.shape)\n",
    "\n",
    "out = torch.split(a, [1, 3, 6], dim=0)\n",
    "for t in out:\n",
    "    display(t, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理多个tensor的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.cat组合两个tensor（不会创建新的维度）\n",
    "a = torch.zeros((2, 4))\n",
    "b = torch.ones((2, 4))\n",
    "out1 = torch.cat((a,b), dim=0)\n",
    "display(out1)\n",
    "out2 = torch.cat((a,b), dim=1)\n",
    "display(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  7.],\n",
       "         [ 2.,  8.],\n",
       "         [ 3.,  9.]],\n",
       "\n",
       "        [[ 4., 10.],\n",
       "         [ 5., 11.],\n",
       "         [ 6., 12.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.stack创建新的维度堆叠tensor\n",
    "a = torch.linspace(1, 6, 6).view(2, 3)\n",
    "b = torch.linspace(7, 12, 6).view(2, 3)\n",
    "display(a, b)\n",
    "out = torch.stack((a, b), dim=2)\n",
    "display(out)\n",
    "display(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3958],\n",
       "        [0.1515]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3623],\n",
       "        [0.5476]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4296)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3975)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3962)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4238)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4030)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4238)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算两个tensor的欧式距离\n",
    "a = torch.rand(2, 1)\n",
    "b = torch.rand(2, 1)\n",
    "display(a, b)\n",
    "display(torch.dist(a, b, p = 1))\n",
    "display(torch.dist(a, b, p = 2))\n",
    "display(torch.dist(a, b, p = 3))\n",
    "\n",
    "display(torch.norm(a))\n",
    "display(torch.norm(a, p=3))\n",
    "display(torch.norm(a, p='fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0532, 5.1024],\n",
       "        [8.7267, 8.8768]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [8., 8.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [9., 9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [9., 9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [8., 8.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0532, 0.1024],\n",
       "        [0.7267, 0.8768]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0532, 1.1024],\n",
       "        [0.7267, 0.8768]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0532, 2.1024],\n",
       "        [0.7267, 3.8768]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0532, 2.1024],\n",
       "        [0.7267, 3.8768]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    torch.floor：将张量中的元素向下取整，得到不大于原值的最大整数。\n",
    "    torch.ceil：将张量中的元素向上取整，得到不小于原值的最小整数。\n",
    "    torch.round：将张量中的元素四舍五入为最接近的整数。\n",
    "    torch.trunc：将张量中的元素截断取整，保留整数部分。\n",
    "    torch.frac：提取张量中元素的小数部分。\n",
    "    torch.fmod：对两个张量的元素进行模运算，返回的结果具有与被除数相同的符号。这意味着在模运算时，结果的符号与被除数的符号保持一致。\n",
    "    torch.remainder：对两个张量的元素进行模运算，返回的结果具有与除数相同的符号。这意味着在模运算时，结果的符号与除数的符号保持一致。\n",
    "\"\"\"\n",
    "a = torch.rand(2, 2)\n",
    "a = a * 10\n",
    "display(a)\n",
    "\n",
    "display(torch.floor(a))\n",
    "display(torch.ceil(a))\n",
    "display(torch.round(a))\n",
    "display(torch.trunc(a))\n",
    "display(torch.frac(a))\n",
    "display(a % 2)\n",
    "b = torch.tensor([[2, 3], [4, 5]], dtype=torch.float)\n",
    "display(torch.fmod(a, b))\n",
    "display(torch.remainder(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0506, 0.3802, 0.9635],\n",
       "        [0.7589, 0.3405, 0.0539]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3014, 0.4968, 1.1735],\n",
       "        [1.3661, 0.4125, 0.6147]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3014, 0.4968, 1.1735],\n",
       "        [1.3661, 0.4125, 0.6147]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3014, 0.4968, 1.1735],\n",
       "        [1.3661, 0.4125, 0.6147]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3014, 0.4968, 1.1735],\n",
       "        [1.3661, 0.4125, 0.6147]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3014, 0.4968, 1.1735],\n",
       "        [1.3661, 0.4125, 0.6147]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor之间相加\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "display(a, b)\n",
    "display(a + b)\n",
    "display(a.add(b))\n",
    "display(torch.add(a, b))\n",
    "display(a)\n",
    "display(a.add_(b))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor相减\n",
    "display(a - b)\n",
    "display(torch.sub(a, b))\n",
    "display(a.sub(b))\n",
    "display(a.sub_(b))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2959, 0.6185]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7581, 0.0541]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2243, 0.0335]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2243, 0.0335]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2243, 0.0335]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2959, 0.6185]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2243, 0.0335]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2243, 0.0335]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor数位相乘\n",
    "a = torch.rand(1, 2)\n",
    "b = torch.rand(1, 2)\n",
    "display(a, b)\n",
    "display(a * b)\n",
    "display(torch.mul(a, b))\n",
    "display(a.mul(b))\n",
    "display(a)\n",
    "display(a.mul_(b))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2508, 0.1166, 0.2100],\n",
       "        [0.6073, 0.0720, 0.5608]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor相除\n",
    "display(a/b)\n",
    "display(torch.div(a, b))\n",
    "display(a.div(b))\n",
    "display(a.div_(b))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3134],\n",
       "        [0.4251]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1687, 0.3888]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0529, 0.1219],\n",
       "        [0.0717, 0.1653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0529, 0.1219],\n",
       "        [0.0717, 0.1653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0529, 0.1219],\n",
       "        [0.0717, 0.1653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0529, 0.1219],\n",
       "        [0.0717, 0.1653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0529, 0.1219],\n",
       "        [0.0717, 0.1653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor的矩阵乘法\n",
    "a = torch.rand(2, 1)\n",
    "b = torch.rand(1, 2)\n",
    "display(a, b)\n",
    "display(a @ b)\n",
    "display(a.matmul(b))\n",
    "display(torch.matmul(a, b))\n",
    "display(torch.mm(a, b))\n",
    "display(a.mm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 多维矩阵相乘，只乘最后两维，但前面维数要一样\n",
    "x = torch.ones(4, 3, 2, 3)\n",
    "y = torch.ones(4, 3, 3, 3)\n",
    "display(x.size(), y.size())\n",
    "c = torch.matmul(x, y)\n",
    "display(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7881, -1.6043,  0.6394],\n",
      "        [ 1.6493,  0.4991, -0.9075]])\n",
      "tensor([[0.6743, 0.4484, 1.3767],\n",
      "        [2.2811, 1.2835, 0.6352]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "a = a.mul(0.5).exp()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor的乘方\n",
    "a = torch.tensor([1, 2])\n",
    "display(torch.pow(a, 3))\n",
    "display(a.pow(3))\n",
    "display(a**3)\n",
    "display(a.pow_(3))\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.7183, 7.3891])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.7183, 7.3891])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([  15.1543, 1618.1781])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([  15.1543, 1618.1781])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exp\n",
    "a = torch.tensor([1, 2],\n",
    "                 dtype=torch.float32)\n",
    "display(a.type())\n",
    "display(torch.exp(a))\n",
    "display(torch.exp_(a))\n",
    "display(a.exp())\n",
    "display(a.exp_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8340, -0.3665])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8340, -0.3665])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log\n",
    "a = torch.tensor([10, 2],\n",
    "                 dtype=torch.float32)\n",
    "display(torch.log(a))\n",
    "display(torch.log_(a))\n",
    "display(a.log())\n",
    "display(a.log_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1623, 1.4142])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.1623, 1.4142])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.7783, 1.1892])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.7783, 1.1892])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sqrt\n",
    "a = torch.tensor([10, 2],\n",
    "                 dtype=torch.float32)\n",
    "display(torch.sqrt(a))\n",
    "display(torch.sqrt_(a))\n",
    "display(a.sqrt())\n",
    "display(a.sqrt_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5415, 0.2105],\n",
       "        [0.0983, 0.0866]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3199, 0.1485])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6398, 0.2971])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0532, 0.0182])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2123)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0451)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0983)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.mode(\n",
       "values=tensor([0.2105, 0.0866]),\n",
       "indices=tensor([1, 1]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensor的统计方法\n",
    "a = torch.rand(2, 2)\n",
    "display(a)\n",
    "display(torch.mean(a, dim=0))\n",
    "display(torch.sum(a, dim=0))\n",
    "display(torch.prod(a, dim=0))\n",
    "\n",
    "display(torch.argmax(a, dim=0))\n",
    "display(torch.argmin(a, dim=0))\n",
    "\n",
    "display(torch.std(a))\n",
    "display(torch.var(a))\n",
    "\n",
    "display(torch.median(a)) # 中位数\n",
    "display(torch.mode(a))   # 众数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.8908, 8.8141],\n",
      "        [1.5951, 8.8134]])\n",
      "tensor([1., 0., 1., 0., 0., 2.])\n"
     ]
    }
   ],
   "source": [
    "# 计算张量中元素的直方图的函数\n",
    "a = torch.rand(2, 2) * 10\n",
    "display(a)\n",
    "display(torch.histc(a, 6, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 1, 0, 2, 1, 2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 非负整数张量中每个值出现的频次的函数。它返回一个张量，其中索引表示数值，而对应的值表示该数值在输入张量中出现的次数\n",
    "a = torch.tensor([0, 1, 2, 1, 0, 2, 1, 2, 2])\n",
    "display(a)\n",
    "display(torch.bincount(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 三角函数\n",
    "a = torch.zeros(2, 3)\n",
    "b = torch.cos(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0525, 0.9850, 0.4007, 0.5261],\n",
       "        [0.5777, 0.3308, 0.8032, 0.5887],\n",
       "        [0.4958, 0.2491, 0.2358, 0.7320],\n",
       "        [0.4340, 0.7606, 0.6815, 0.9837]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4426, 0.4344, 0.1903, 0.7261],\n",
       "        [0.2721, 0.8383, 0.9600, 0.7061],\n",
       "        [0.8926, 0.4639, 0.9388, 0.3535],\n",
       "        [0.1596, 0.8712, 0.8780, 0.6849]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4426, 0.9850, 0.1903, 0.5261],\n",
       "        [0.5777, 0.8383, 0.8032, 0.5887],\n",
       "        [0.8926, 0.4639, 0.9388, 0.7320],\n",
       "        [0.1596, 0.7606, 0.6815, 0.9837]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.where进行条件判断\n",
    "a = torch.rand(4, 4)\n",
    "b = torch.rand(4, 4)\n",
    "display(a, b)\n",
    "out = torch.where(a > 0.5, a, b)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4222, 0.0403, 0.1243, 0.7992],\n",
       "        [0.3825, 0.1122, 0.8113, 0.8258],\n",
       "        [0.6755, 0.4806, 0.8409, 0.1232],\n",
       "        [0.7096, 0.7113, 0.2379, 0.4624]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4222, 0.0403, 0.1243, 0.7992],\n",
       "        [0.7096, 0.7113, 0.2379, 0.4624],\n",
       "        [0.6755, 0.4806, 0.8409, 0.1232]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据索引选择tensor\n",
    "a = torch.rand(4, 4)\n",
    "display(a)\n",
    "out = torch.index_select(a, dim=0,\n",
    "                   index=torch.tensor([0, 3, 2]))\n",
    "\n",
    "display(out, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.],\n",
       "        [ 9., 10., 11., 12.],\n",
       "        [13., 14., 15., 16.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  6.,  7.,  8.],\n",
       "        [ 1.,  6., 11., 12.],\n",
       "        [ 1.,  6., 15., 16.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据给定的索引从输入张量中收集指定位置的元素\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "display(a)\n",
    "out = torch.gather(a, dim=0,\n",
    "             index=torch.tensor([[0, 1, 1, 1],\n",
    "                                 [0, 1, 2, 2],\n",
    "                                 [0, 1, 3, 3]]))\n",
    "display(out, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.],\n",
       "        [ 9., 10., 11., 12.],\n",
       "        [13., 14., 15., 16.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 10., 11., 12., 13., 14., 15., 16.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据掩码选择tensor中的值\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "mask = torch.gt(a, 8)\n",
    "display(a)\n",
    "display(mask)\n",
    "out = torch.masked_select(a, mask)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.],\n",
       "        [ 9., 10., 11., 12.],\n",
       "        [13., 14., 15., 16.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1., 16., 14., 11.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据索引选择tensor的值(将输入张量扁平化为一维，然后根据索引获取元素。)\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "b = torch.take(a, index=torch.tensor([0, 15, 13, 10]))\n",
    "display(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 0],\n",
       "        [2, 3, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 找到张量中非零元素的索引。它返回一个张量，其中每一行都包含一个非零元素的索引\n",
    "a = torch.tensor([[0, 1, 2, 0], [2, 3, 0, 1]])\n",
    "out = torch.nonzero(a)\n",
    "display(a, out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n一个简单的查找表（lookup table），存储固定字典和大小的词嵌入。\\n此模块通常用于存储单词嵌入并使用索引检索它们(类似数组)。模块的输入是一个索引列表，输出是相应的词嵌入。\\nnum_embeddings - 词嵌入字典大小，即一个字典里要有多少个词。\\nembedding_dim - 每个词嵌入向量的大小。\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "一个简单的查找表（lookup table），存储固定字典和大小的词嵌入。\n",
    "此模块通常用于存储单词嵌入并使用索引检索它们(类似数组)。模块的输入是一个索引列表，输出是相应的词嵌入。\n",
    "num_embeddings - 词嵌入字典大小，即一个字典里要有多少个词。\n",
    "embedding_dim - 每个词嵌入向量的大小。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置一个字典长10，词向量长度为3的词嵌入\n",
    "embedding = torch.nn.Embedding(10, 3)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0970, -2.3749,  1.6615],\n",
       "         [-1.0845,  0.4108,  0.2650],\n",
       "         [ 0.4780,  0.4143,  0.6528],\n",
       "         [ 0.2218,  0.7237, -2.5136]],\n",
       "\n",
       "        [[ 0.4780,  0.4143,  0.6528],\n",
       "         [ 0.9073,  0.4980,  0.4852],\n",
       "         [-1.0845,  0.4108,  0.2650],\n",
       "         [-0.0905,  0.3385, -0.0089]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数字是样本编号索引，每一个是一个长度为3的索引\n",
    "# [1,2,4,5]表示一个句子列表，数字表示词\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.0514e+00,  5.8627e-04, -4.3436e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 1.3114e+00, -1.3614e+00,  1.0755e+00]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example with padding_idx\n",
    "embedding = torch.nn.Embedding(10, 3, padding_idx=0)\n",
    "input1 = torch.LongTensor([[0, 2, 0, 5]])\n",
    "embedding(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [-0.6744,  0.4043,  0.8073],\n",
       "        [ 0.2897,  0.6995,  0.3052]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of changing `pad` vector\n",
    "embedding = torch.nn.Embedding(3, 3, padding_idx=0)\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3360,  1.0078,  0.5159],\n",
      "        [-0.8106,  0.3944, -1.5112],\n",
      "        [ 0.5663, -1.3853,  0.1288]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 进行权重预训练（初始为高斯分布权重）\n",
    "embedding = torch.nn.init.normal_(embedding.weight, 0, 1)\n",
    "print(embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n概率不激活神经元\\np:表示不激活的概率\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "概率不激活神经元\n",
    "p:表示不激活的概率\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5549,  0.1655, -0.1474],\n",
      "        [-0.0985, -0.8826, -0.4346],\n",
      "        [ 0.0348,  0.6007,  0.5137],\n",
      "        [ 1.1078,  0.6560,  1.9803]])\n",
      "tensor([[-0.0000,  0.0000, -0.2105],\n",
      "        [-0.1407, -1.2608, -0.6208],\n",
      "        [ 0.0497,  0.8581,  0.0000],\n",
      "        [ 1.5826,  0.9371,  2.8289]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Dropout(p=0.3)\n",
    "input = torch.randn(4, 3)\n",
    "print(input)\n",
    "print(m(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2949, -0.3116,  0.6805,  ...,  0.1820, -1.4174,  0.2907],\n",
       "         [ 1.2465, -0.9830, -0.1022,  ...,  0.2662,  0.5109, -0.5642],\n",
       "         [-0.0626, -0.3004,  0.0111,  ..., -0.0970, -0.2090,  0.3865],\n",
       "         ...,\n",
       "         [ 0.2609,  0.3006,  0.4918,  ...,  0.2009,  0.2093,  0.3688],\n",
       "         [-0.4449, -0.1258, -0.6255,  ...,  0.2226, -0.9251, -0.1009],\n",
       "         [ 0.7110,  0.2450, -0.8216,  ..., -0.7978,  0.0033, -0.1864]],\n",
       "\n",
       "        [[ 0.9564,  0.2986,  0.6767,  ..., -0.2930,  0.0050,  0.0429],\n",
       "         [-0.2552,  0.3730, -0.2167,  ..., -0.0464,  1.2453,  0.4323],\n",
       "         [ 0.2728,  0.5488,  0.0951,  ..., -0.5268,  1.1085,  0.1429],\n",
       "         ...,\n",
       "         [-0.1823,  0.6875,  1.2736,  ...,  0.8126,  0.6698,  0.2496],\n",
       "         [-0.2675, -0.2753,  0.2984,  ..., -0.2252,  0.2771,  0.4653],\n",
       "         [-0.5681,  0.2002,  0.1237,  ...,  0.2349, -1.1563, -0.6949]],\n",
       "\n",
       "        [[-1.1437, -0.6525,  0.6027,  ..., -0.1978,  0.1055, -0.8331],\n",
       "         [-0.0048, -0.5421,  0.4392,  ...,  0.0430, -0.6165,  0.0446],\n",
       "         [ 0.0349, -0.1723,  0.1155,  ...,  0.0035, -0.6296, -0.9501],\n",
       "         ...,\n",
       "         [-0.5574, -0.0854, -0.4557,  ...,  0.5077,  1.5400, -0.6197],\n",
       "         [ 0.1534,  0.4932,  0.0022,  ..., -0.1554, -0.4370,  0.3775],\n",
       "         [-0.2425,  0.2490, -0.0033,  ..., -0.5057, -1.1678,  0.8620]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.2888, -0.9460,  0.1075,  ...,  0.9177, -0.2289,  0.8830],\n",
       "         [-0.1285, -0.2916,  0.3886,  ...,  0.5445,  0.9315, -0.4245],\n",
       "         [ 0.5478, -0.7592,  0.2170,  ..., -0.4184, -0.5007,  0.2840],\n",
       "         ...,\n",
       "         [ 0.1471, -0.2660, -0.6476,  ...,  0.7606,  0.4774,  0.2098],\n",
       "         [-0.2151,  0.4431,  0.0268,  ...,  0.5969, -0.6846,  0.4712],\n",
       "         [ 0.9923,  0.0076,  0.0779,  ...,  0.0028, -0.2462, -0.3631]],\n",
       "\n",
       "        [[ 0.2582,  0.6664, -0.6055,  ...,  0.4564,  0.4037, -0.4660],\n",
       "         [ 0.2013,  0.1860, -0.7452,  ...,  0.2205,  0.0593,  0.0494],\n",
       "         [-0.1405, -0.1167, -0.0551,  ...,  1.1307,  0.5248, -1.0820],\n",
       "         ...,\n",
       "         [ 0.3979,  0.0857, -0.4513,  ...,  0.3959, -0.0078, -0.1857],\n",
       "         [-0.1045, -0.0156,  0.1182,  ...,  0.4699, -0.0226, -0.5430],\n",
       "         [-0.1490, -0.3810,  0.0637,  ...,  0.1466,  0.1395, -1.1514]],\n",
       "\n",
       "        [[-0.4234, -0.7054, -0.1202,  ...,  0.1228,  0.7137,  0.5867],\n",
       "         [-0.1767, -0.4532,  0.3250,  ...,  0.3595,  0.0606, -0.6000],\n",
       "         [-0.8435,  0.2815, -0.4679,  ..., -0.3346,  0.0086, -0.3119],\n",
       "         ...,\n",
       "         [-0.3651, -0.0785,  0.6001,  ..., -0.4208, -0.0403, -0.5319],\n",
       "         [-0.0394,  0.0579,  0.4456,  ...,  0.1540, -0.2524, -0.1701],\n",
       "         [ 0.5012,  0.2683, -0.1321,  ...,  0.0643,  0.6944,  0.6031]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)  # 输入16，输出33\n",
    "input = torch.randn(20, 16, 50)  # 批量为20，样本个数为16，长度为50的词向量\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.2543e-01, -6.4423e-01,  5.7986e-01, -6.2900e-01],\n",
       "         [ 5.4330e-01,  1.9020e-01, -4.0227e-01,  1.2573e-01],\n",
       "         [ 1.6384e-01,  5.7504e-02,  5.4488e-02, -9.7998e-02],\n",
       "         [ 4.1489e-01, -3.4379e-01, -3.5539e-01,  9.9075e-01]],\n",
       "\n",
       "        [[-1.5087e+00, -2.2464e-01,  2.6727e-02, -4.2099e-01],\n",
       "         [ 1.5470e-01, -6.6959e-01, -2.5230e-01, -5.7251e-01],\n",
       "         [-1.2495e-01,  2.5413e-01, -2.9930e-02,  4.8517e-02],\n",
       "         [ 3.8806e-02, -8.9407e-01,  1.6918e-01,  4.8433e-05]],\n",
       "\n",
       "        [[ 2.4978e-02, -8.4620e-01, -1.7038e+00, -1.3812e-01],\n",
       "         [-3.9297e-01, -3.7726e-01, -2.1093e-02, -4.9845e-01],\n",
       "         [-3.6471e-01, -3.3163e-01, -6.0916e-01,  1.4997e-02],\n",
       "         [-4.3039e-01, -2.1968e-01,  5.2735e-02, -9.8434e-02]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = torch.nn.Conv1d(5, 4, 2)  # kernel_size = 2\n",
    "input = torch.randn(3, 5, 5)\n",
    "n(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "key = torch.randn(3, 4)\n",
    "print(key.size())\n",
    "print(*key.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "time_steps = 3\n",
    "embedding_dim = 4\n",
    "inputx = torch.randn(batch_size, time_steps, embedding_dim)  # N*L*C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch中的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3666, 0.5533],\n",
       "        [0.4492, 0.2403]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2259, 0.1686],\n",
       "        [0.4962, 0.7516]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7682, 0.5979],\n",
       "        [0.3391, 0.7125]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2259, 0.1686],\n",
       "        [0.4962, 0.7516]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3666, 0.5533],\n",
       "        [0.4492, 0.2403]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pytorch 自定义autograd\n",
    "class line(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # 正向传播\n",
    "    def forward(ctx, w, x, b):\n",
    "        ctx.save_for_backward(w, x, b)\n",
    "        return w * x + b\n",
    "\n",
    "    @staticmethod\n",
    "    # 反向传播\n",
    "    def backward(ctx, grad_out):\n",
    "        w, x, b = ctx.saved_tensors\n",
    "        grad_w = grad_out * x\n",
    "        grad_x = grad_out * w\n",
    "        grad_b = grad_out\n",
    "\n",
    "        return grad_w, grad_x, grad_b\n",
    "\n",
    "\n",
    "w = torch.rand(2, 2, requires_grad=True)\n",
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "b = torch.rand(2, 2, requires_grad=True)\n",
    "\n",
    "# 正向传播得到out\n",
    "out = line.apply(w, x, b)\n",
    "out.backward(torch.ones(2, 2))\n",
    "\n",
    "display(w, x, b)\n",
    "display(w.grad, x.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[18., 18.],\n",
       "         [18., 18.]]),\n",
       " tensor([[18., 18.],\n",
       "         [18., 18.]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[72., 72.],\n",
       "        [72., 72.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pytorch中grad的使用\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x.register_hook(lambda grad:grad*2)\n",
    "\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "\n",
    "nn = torch.ones(2, 2)\n",
    "display(nn)\n",
    "\n",
    "# nn作为梯度的乘法因子\n",
    "z.backward(gradient = nn, retain_graph=True)\n",
    "torch.autograd.backward(z, grad_tensors=nn, retain_graph=True)\n",
    "\n",
    "display(torch.autograd.grad(z, [x, y, z], grad_outputs=nn))\n",
    "display(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hub模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1346\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1347\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1348\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mConnect to a host on a given (SSL) port.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1447\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:946\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[39m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[0;32m    947\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[0;32m    948\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\socket.py:844\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    845\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    846\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\socket.py:832\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 832\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    833\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\code\\jupyter_code\\深度学习\\pytorch1.ipynb 单元格 109\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/code/jupyter_code/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1.ipynb#Y264sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mpytorch/vision\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdeeplabv3_resnet101\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/code/jupyter_code/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1.ipynb#Y264sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:539\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    536\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown source: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Allowed values: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 539\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[39m\"\u001b[39;49m\u001b[39mload\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    540\u001b[0m                                        verbose\u001b[39m=\u001b[39;49mverbose, skip_validation\u001b[39m=\u001b[39;49mskip_validation)\n\u001b[0;32m    542\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    543\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:203\u001b[0m, in \u001b[0;36m_get_cache_or_reload\u001b[1;34m(github, force_reload, trust_repo, calling_fn, verbose, skip_validation)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39m# Validate the tag/branch is from the original repo instead of a forked repo\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_validation:\n\u001b[1;32m--> 203\u001b[0m         _validate_not_a_forked_repo(repo_owner, repo_name, ref)\n\u001b[0;32m    205\u001b[0m     cached_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hub_dir, normalized_br \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    206\u001b[0m     _remove_if_exists(cached_file)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:162\u001b[0m, in \u001b[0;36m_validate_not_a_forked_repo\u001b[1;34m(repo_owner, repo_name, ref)\u001b[0m\n\u001b[0;32m    160\u001b[0m page \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    161\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00murl_prefix\u001b[39m}\u001b[39;00m\u001b[39m?per_page=100&page=\u001b[39m\u001b[39m{\u001b[39;00mpage\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 162\u001b[0m response \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(_read_url(Request(url, headers\u001b[39m=\u001b[39;49mheaders)))\n\u001b[0;32m    163\u001b[0m \u001b[39m# Empty response means no more data to process\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m response:\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:145\u001b[0m, in \u001b[0;36m_read_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_url\u001b[39m(url):\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mwith\u001b[39;00m urlopen(url) \u001b[39mas\u001b[39;00m r:\n\u001b[0;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mdecode(r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget_content_charset(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    519\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1390\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:1349\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[0;32m   1347\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1350\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m   1351\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\code\\jupyter_code\\深度学习\\pytorch1.ipynb 单元格 110\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/code/jupyter_code/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1.ipynb#Y265sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/code/jupyter_code/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1.ipynb#Y265sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mpytorch/vision\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39malexnet\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/code/jupyter_code/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1.ipynb#Y265sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:539\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    536\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown source: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Allowed values: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 539\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[39m\"\u001b[39;49m\u001b[39mload\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    540\u001b[0m                                        verbose\u001b[39m=\u001b[39;49mverbose, skip_validation\u001b[39m=\u001b[39;49mskip_validation)\n\u001b[0;32m    542\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    543\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:180\u001b[0m, in \u001b[0;36m_get_cache_or_reload\u001b[1;34m(github, force_reload, trust_repo, calling_fn, verbose, skip_validation)\u001b[0m\n\u001b[0;32m    178\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(hub_dir)\n\u001b[0;32m    179\u001b[0m \u001b[39m# Parse github repo information\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m repo_owner, repo_name, ref \u001b[39m=\u001b[39m _parse_repo_info(github)\n\u001b[0;32m    181\u001b[0m \u001b[39m# Github allows branch name with slash '/',\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m# this causes confusion with path on both Linux and Windows.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m# Backslash is not allowed in Github branch name so no need to\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m# to worry about it.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m normalized_br \u001b[39m=\u001b[39m ref\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:134\u001b[0m, in \u001b[0;36m_parse_repo_info\u001b[1;34m(github)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m ref \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[39m# The ref wasn't specified by the user, so we need to figure out the\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     \u001b[39m# default branch: main or master. Our assumption is that if main exists\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39m# then it's the default branch, otherwise it's master.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mwith\u001b[39;00m urlopen(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://github.com/\u001b[39;49m\u001b[39m{\u001b[39;49;00mrepo_owner\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mrepo_name\u001b[39m}\u001b[39;49;00m\u001b[39m/tree/main/\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m    135\u001b[0m             ref \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    136\u001b[0m     \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    519\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1390\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\urllib\\request.py:1350\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m-> 1350\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m   1351\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1352\u001b[0m     h\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\IDE_sources\\Ancona\\envs\\pytorch\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_normalizion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个批量进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2764,  1.2371, -1.1694,  1.9056],\n",
      "         [-0.1912, -0.2005, -0.6737,  0.4105],\n",
      "         [-0.6522, -0.0208, -1.2549, -0.1947]],\n",
      "\n",
      "        [[ 0.5905,  0.6494,  0.8876,  1.0624],\n",
      "         [ 0.3439, -0.1319, -2.0763, -1.5370],\n",
      "         [-0.2877,  0.8809, -2.4044,  1.9477]]])\n",
      "tensor([[[ 0.6180,  1.5178, -0.0509,  1.0686],\n",
      "         [-0.4803, -1.0962,  0.4143, -0.1543],\n",
      "         [-1.5635, -0.7694, -0.1311, -0.6492]],\n",
      "\n",
      "        [[ 1.3561,  0.4491,  1.8793,  0.3790],\n",
      "         [ 0.7768, -0.9714, -0.9019, -1.7471],\n",
      "         [-0.7072,  0.8701, -1.2097,  1.1030]]])\n"
     ]
    }
   ],
   "source": [
    "# 调用batcch_norm API\n",
    "# number of features or channels CC of the input\n",
    "batch_norm_op = torch.nn.BatchNorm1d(embedding_dim, affine=False)  # 采用无偏归一化\n",
    "bn_y = batch_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)\n",
    "print(inputx)\n",
    "print(bn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4218,  0.2444, -1.6615,  1.0068],\n",
      "         [-0.5583, -0.4160,  0.7688, -0.2923],\n",
      "         [-0.7681, -0.3263,  0.3474, -1.2080]],\n",
      "\n",
      "        [[-0.1323, -0.1247, -0.0521, -0.4196],\n",
      "         [ 0.2027,  0.0709,  0.4221, -0.5306],\n",
      "         [ 0.4568, -2.0859,  2.2416,  2.0668]]])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 4])\n",
      "tensor([[-0.5827, -0.1659, -0.1818, -0.1645],\n",
      "        [ 0.1757, -0.7132,  0.8705,  0.3722]])\n"
     ]
    }
   ],
   "source": [
    "# 均值和方差\n",
    "a = torch.randn(2, 3, 4)\n",
    "print(a)\n",
    "print(a.size())\n",
    "b = a.mean((1), keepdim=False)  # 将第一维的所有数据相加求均值\n",
    "print(b.size())\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均值和方差"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均值和方差计算时计算某个维度的均值，该维度中所有的值都变为一个，其他维度不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1923,  0.6503,  0.0430, -0.8843],\n",
      "         [ 0.4187,  1.1993,  0.2637,  0.7527],\n",
      "         [-0.4696, -0.2032,  1.4352, -0.5672]],\n",
      "\n",
      "        [[-0.8833, -0.3175, -0.0191,  0.7252],\n",
      "         [ 1.1991, -1.6889,  0.1586, -1.1105],\n",
      "         [-1.8764,  0.6856, -0.6121,  0.2912]]])\n",
      "tensor([[0.4545, 0.7067, 0.7483, 0.8682],\n",
      "        [1.5696, 1.1920, 0.4036, 0.9594]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "print(a)\n",
    "b = a.std((1))\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6180,  1.5178, -0.0509,  1.0686],\n",
      "         [-0.4803, -1.0962,  0.4143, -0.1543],\n",
      "         [-1.5635, -0.7694, -0.1311, -0.6492]],\n",
      "\n",
      "        [[ 1.3561,  0.4491,  1.8793,  0.3790],\n",
      "         [ 0.7768, -0.9714, -0.9019, -1.7471],\n",
      "         [-0.7072,  0.8701, -1.2097,  1.1030]]])\n",
      "tensor([[[ 0.6180,  1.5178, -0.0509,  1.0686],\n",
      "         [-0.4803, -1.0961,  0.4143, -0.1543],\n",
      "         [-1.5635, -0.7694, -0.1311, -0.6492]],\n",
      "\n",
      "        [[ 1.3561,  0.4491,  1.8793,  0.3790],\n",
      "         [ 0.7768, -0.9714, -0.9019, -1.7471],\n",
      "         [-0.7072,  0.8701, -1.2097,  1.1030]]])\n"
     ]
    }
   ],
   "source": [
    "# 实现batch_norm\n",
    "bn_mean = inputx.mean((0, 1), keepdim=True)\n",
    "bn_std = inputx.std((0, 1), keepdim=True, unbiased=False)\n",
    "vertify_bn_y = (inputx - bn_mean) / (bn_std + 1e-5)\n",
    "print(bn_y)\n",
    "print(vertify_bn_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每一个样本进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api实现lay_norm\n",
    "layer_norm_op = torch.nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "ln_y = layer_norm_op(inputx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "tensor([[[-0.2016, -1.5521,  0.9563,  0.7974],\n",
      "         [ 1.5631,  0.0544, -0.4585, -1.1590],\n",
      "         [-0.2273,  1.6875, -0.8612, -0.5990]],\n",
      "\n",
      "        [[ 0.7062, -0.9074, -1.0521,  1.2534],\n",
      "         [-0.3024,  0.5005, -1.4478,  1.2497],\n",
      "         [-1.0904, -0.6556,  1.5275,  0.2185]]])\n",
      "tensor([[[-0.2016, -1.5521,  0.9563,  0.7974],\n",
      "         [ 1.5631,  0.0544, -0.4585, -1.1590],\n",
      "         [-0.2273,  1.6875, -0.8612, -0.5990]],\n",
      "\n",
      "        [[ 0.7062, -0.9074, -1.0521,  1.2534],\n",
      "         [-0.3024,  0.5005, -1.4478,  1.2497],\n",
      "         [-1.0904, -0.6556,  1.5275,  0.2185]]])\n"
     ]
    }
   ],
   "source": [
    "# 实现layer_norm\n",
    "ln_mean = inputx.mean(dim=-1, keepdim=True)  # keepdim表示保持维度不会减少\n",
    "print(ln_mean.size())\n",
    "ln_std = inputx.std(dim=-1, keepdim=True, unbiased=False)\n",
    "vertify_ln_y = (inputx - ln_mean) / (ln_std + 1e-5)\n",
    "print(ln_y)\n",
    "print(vertify_ln_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个样本的每个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\n",
    "instance_norm_op = torch.nn.InstanceNorm1d(embedding_dim)\n",
    "in_y = instance_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2190, -1.2575,  0.9567,  1.1196],\n",
      "         [ 1.3195,  1.1891,  0.4236,  0.1884],\n",
      "         [-1.1005,  0.0683, -1.3803, -1.3080]],\n",
      "\n",
      "        [[ 0.8484, -0.5451, -0.7319,  0.2044],\n",
      "         [ 0.5556,  1.4027, -0.6820,  1.1097],\n",
      "         [-1.4041, -0.8575,  1.4139, -1.3141]]])\n",
      "tensor([[[-0.2190, -1.2575,  0.9567,  1.1196],\n",
      "         [ 1.3195,  1.1891,  0.4236,  0.1884],\n",
      "         [-1.1005,  0.0683, -1.3803, -1.3080]],\n",
      "\n",
      "        [[ 0.8484, -0.5451, -0.7319,  0.2044],\n",
      "         [ 0.5556,  1.4026, -0.6820,  1.1097],\n",
      "         [-1.4041, -0.8575,  1.4139, -1.3141]]])\n"
     ]
    }
   ],
   "source": [
    "# 动手\n",
    "in_mean = inputx.mean(dim=1, keepdim=True)\n",
    "in_std = inputx.std(dim=1, keepdim=True, unbiased=False)\n",
    "vertify_in_y = (inputx - in_mean) / (in_std + 1e-5)\n",
    "print(in_y)\n",
    "print(vertify_in_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个样本的每个组进行归一化（要对样本向量进行分组）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\n",
    "num_group = 2\n",
    "group_norm_op = torch.nn.GroupNorm(num_group, embedding_dim, affine=False)\n",
    "gn_y = group_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0331, -1.2634,  1.1040,  0.9467],\n",
      "         [ 1.6584,  0.7907,  0.5323,  0.0944],\n",
      "         [-1.0022, -0.1503, -1.4021, -1.2754]],\n",
      "\n",
      "        [[ 0.8322, -0.6144, -1.0542,  0.7982],\n",
      "         [ 0.5579,  1.4468, -1.0236,  1.6527],\n",
      "         [-1.2775, -0.9450,  0.2620, -0.6351]]])\n",
      "tensor([[[-0.0331, -1.2634,  1.1040,  0.9467],\n",
      "         [ 1.6584,  0.7907,  0.5323,  0.0944],\n",
      "         [-1.0022, -0.1503, -1.4021, -1.2754]],\n",
      "\n",
      "        [[ 0.8322, -0.6144, -1.0542,  0.7982],\n",
      "         [ 0.5579,  1.4468, -1.0236,  1.6527],\n",
      "         [-1.2775, -0.9450,  0.2620, -0.6351]]])\n"
     ]
    }
   ],
   "source": [
    "# 动手\n",
    "groups_inputx = torch.split(\n",
    "    inputx, split_size_or_sections=embedding_dim // num_group, dim=-1\n",
    ")\n",
    "result = []\n",
    "for g_inputx in groups_inputx:\n",
    "    gn_mean = g_inputx.mean(dim=(1, 2), keepdim=True)\n",
    "    gn_std = g_inputx.std(dim=(1, 2), keepdim=True, unbiased=False)\n",
    "    gn_result = (g_inputx - gn_mean) / (gn_std + 1e-5)\n",
    "    result.append(gn_result)\n",
    "vertify_gn_y = torch.cat(result, dim=-1)\n",
    "print(gn_y)\n",
    "print(vertify_gn_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "2 3 3\n"
     ]
    }
   ],
   "source": [
    "# api\n",
    "linear = nn.Linear(embedding_dim, 3, bias=False)\n",
    "wn_linear = torch.nn.utils.weight_norm(linear)\n",
    "wn_linear_output = wn_linear(inputx)\n",
    "print(wn_linear_output.shape)\n",
    "print(*wn_linear_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "tensor([[[-0.6211, -1.2264, -0.2623],\n",
      "         [-0.3892, -0.8806, -0.4603],\n",
      "         [ 0.3847,  0.6419,  0.1565]],\n",
      "\n",
      "        [[-0.1541, -0.2958, -0.4514],\n",
      "         [ 0.5465, -0.4089, -0.7363],\n",
      "         [-0.0882,  0.0252,  0.3594]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 动手\n",
    "print(linear.weight.shape)\n",
    "weight_direction = linear.weight / (linear.weight.norm(dim=1, keepdim=True))\n",
    "weight_magnitude = wn_linear.weight_g\n",
    "print(weight_direction.shape)\n",
    "print(weight_magnitude.shape)\n",
    "vertify_wn_output = (\n",
    "    inputx @ (weight_direction.transpose(-1, -2)) * (weight_magnitude.transpose(-1, -2))\n",
    ")\n",
    "print(wn_linear_output)\n",
    "print(vertify_wn_output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点积和乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2]])\n",
      "tensor([[2, 2, 2],\n",
      "        [2, 2, 1]])\n",
      "tensor([[2, 2, 2],\n",
      "        [4, 4, 2]])\n",
      "tensor([[ 6,  5],\n",
      "        [12, 10]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(1, 3, (2, 3))\n",
    "b = torch.randint(1, 3, (2, 3))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a * b)  # 矩阵相应位置进行乘积\n",
    "print(a @ b.transpose(-1, -2))  # 矩阵的点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "num_classes = 3\n",
    "model = LinearClassifier(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8231\n",
      "Epoch [20/100], Loss: 0.4045\n",
      "Epoch [30/100], Loss: 0.4878\n",
      "Epoch [40/100], Loss: 0.5430\n",
      "Epoch [50/100], Loss: 0.5410\n",
      "Epoch [60/100], Loss: 0.4593\n",
      "Epoch [70/100], Loss: 0.2255\n",
      "Epoch [80/100], Loss: 0.5825\n",
      "Epoch [90/100], Loss: 0.4099\n",
      "Epoch [100/100], Loss: 0.2092\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 90.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
