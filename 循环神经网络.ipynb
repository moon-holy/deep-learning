{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size = 10, hidden_size = 20, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# 第一层权重系数\n",
    "print(rnn.weight_ih_l0.shape)\n",
    "print(rnn.weight_hh_l0.shape)\n",
    "print(rnn.bias_ih_l0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win形状 torch.Size([20, 20])\n",
      "whh形状 torch.Size([20, 20])\n",
      "bih形状 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# 第二层权重系数\n",
    "print(\"win形状 {}\".format(rnn.weight_ih_l1.shape))\n",
    "print(\"whh形状 {}\".format(rnn.weight_hh_l1.shape))\n",
    "print(\"bih形状 {}\".format(rnn.bias_ih_l1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "# 输入特征长度100，批次32，特征维度10\n",
    "input = torch.randn(100, 32, 10)\n",
    "# 隐状态2层，32批次，维度20\n",
    "h_0 = torch.randn(2, 32, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 20]) torch.Size([2, 32, 20])\n"
     ]
    }
   ],
   "source": [
    "output, h_n = rnn(input, h_0)\n",
    "print(output.shape, h_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pytorch构建RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2o = nn.Linear(input_size+hidden_size, output_size)\n",
    "        self.i2h = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim = 1)\n",
    "        output = self.i2o(combined)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens = 128\n",
    "rnn = RNN(10, n_hiddens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2o): Linear(in_features=138, out_features=10, bias=True)\n",
      "  (i2h): Linear(in_features=138, out_features=128, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size = 10, hidden_size = 20, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(10, 20, num_layers=2)\n"
     ]
    }
   ],
   "source": [
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wih形状 torch.Size([80, 10]) \n",
      "whh形状 torch.Size([80, 20]) \n",
      "bih形状 torch.Size([80]) \n"
     ]
    }
   ],
   "source": [
    "print(\"wih形状 {} \".format(lstm.weight_ih_l0.shape))\n",
    "print(\"whh形状 {} \".format(lstm.weight_hh_l0.shape))\n",
    "print(\"bih形状 {} \".format(lstm.bias_ih_l0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wih形状 torch.Size([80, 20]) \n",
      "whh形状 torch.Size([80, 20]) \n",
      "bih形状 torch.Size([80]) \n"
     ]
    }
   ],
   "source": [
    "print(\"wih形状 {} \".format(lstm.weight_ih_l1.shape))\n",
    "print(\"whh形状 {} \".format(lstm.weight_hh_l1.shape))\n",
    "print(\"bih形状 {} \".format(lstm.bias_ih_l1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "# 输入特征长度100，批次32，特征维度10\n",
    "input = torch.randn(100, 32, 10)\n",
    "# 隐状态2层，32批次，维度20\n",
    "h_0 = torch.randn(2, 32, 20)\n",
    "h0 = (h_0, h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 20]) torch.Size([2, 32, 20]) torch.Size([2, 32, 20])\n"
     ]
    }
   ],
   "source": [
    "output, h_n = lstm(input, h0)\n",
    "print(output.size(), h_n[0].size(), h_n[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# 测试linear\n",
    "liear = nn.Linear(10, 5)\n",
    "print(liear.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pytorch实现LSTM\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, cell_size, output_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "        self.gate = nn.Linear(input_size+hidden_size, cell_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        f_gate = self.sigmoid(self.gate(combined))  # 遗忘门\n",
    "        o_gate = self.sigmoid(self.gate(combined))  # 输出门\n",
    "        i_gate = self.sigmoid(self.gate(combined))  # 输入门\n",
    "        c_state = self.tanh(self.gate(combined))     # 局部留存\n",
    "        cell = torch.add(torch.mul(cell, f_gate), torch.mul(i_gate, c_state))   # 内部状态\n",
    "        hidden = torch.mul(o_gate, self.tanh(cell))    # 隐状态\n",
    "        output = self.output(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def initCell(self):\n",
    "        return torch.zeros(1, self.cell_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmcell = LSTMCell(input_size=10, hidden_size=20, cell_size=20, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 10)\n",
    "h_0 = torch.randn(32, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 20]) torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "output, h_n, c_n = lstmcell(input, h_0, h_0)\n",
    "print(output.size(), h_n.size(), c_n.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.gate = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        z_gate = self.sigmoid(self.gate(combined)) # 更新门\n",
    "        r_gate = self.sigmoid(self.gate(combined)) # 重置门\n",
    "        combined01 = torch.cat((input, torch.mul(hidden, r_gate)), dim=1)\n",
    "        h1_state = self.tanh(self.gate(combined01)) # 候选状态\n",
    "        h_state = torch.add(torch.mul(z_gate, h1_state), torch.mul((1-z_gate), hidden))\n",
    "        output = self.softmax(self.output(h_state))\n",
    "        return output, h_state\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grucell = GRUCell(input_size=10, hidden_size=20, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 10)\n",
    "h_0 = torch.randn(32, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "output, h_n = grucell(input, h_0)\n",
    "print(output.size(), h_n.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
